# 256-add-rlm-integration-to-dialtone-agent
### signature:
- status: wait
- issue: 256
- source: github
- url: https://github.com/timcash/dialtone/issues/256
- synced-at: 2026-02-21T19:50:23Z
### sync:
- github-updated-at: 2026-02-14T21:18:54Z
- last-pulled-at: 2026-02-21T19:50:23Z
- last-pushed-at: 
- github-labels-hash: 
### description:
- https://arxiv.org/pdf/2512.24601
- common misconceptions
- question - what stops sub calls from overloading the context of its parent? better prompt engineering?
- Great question! In RLMs, the individual steps are *not allowed* to return long outputs into the root.
- The scaffold forces the output to be short/truncated to force the root LLM to rely on recursion.
- ![image](https://github.com/user-attachments/assets/6dda09ec-6dfd-4e9e-80b1-fca663e140a4)
- RLMs are not sub-agents or the ability to iteratively retrieve context. I know because I trained multi-hop models for reasoning & retrieval in 2020, including compaction.*
- RLMs are the simplest/purest scaffold that understands its own prompts via recursion, not via attention.
- They support an extremely simple but unusual claim: Models need to be able to access their own conversations with the user and their own horizon symbolically and recursively.
- The model should be only allowed to understand this long context by *writing code* that launches LLMs, and composing these into the final response.
- Note that the number of LLM launches can be linear or even bigger in the context size, not a small constant number of sub-tasks. This sounds big until you remember that attention is already quadratic.
- I'll have to confess that I always found (and still find) the conventional pattern of "sub-agents" rather boring. This is the superficially related structure where the model is given a special tool it can invoke by writing out prompts for and receiving the output.
- Verbalizing specific individual sub-calls as tool calls token-by-token hides the internal reasoning from the main context, which is an OK outcome for sub-task delegation. But it's a completely unrelated pattern to teaching models to understand their own context/horizon recursively.
- Sorry I'm a bit of a pedant for understanding concepts precisely, but this seemed needed.
- *The title is quite literally "Robust Multi-Hop Reasoning at Scale via Condensed Retrieval", arXived on Jan 2nd 2021. It could work for many steps, retrieve text from a massive corpus, compact/condense its own context, and iterate further
- # compare to this line of thinking 
- The Main Problem
- The core problem they're solving is how to make RL work at massive scale for being good at Agentic AI. 
- It is is about training LLMs in a way so that they can become good agents.
- Agents are not about generating one response from a query. 
- LLMs that are good at being agents must learn how to take actions, use arbitrary tools, manage long contexts, and interact with complex environments as it's performing tasks.
- Training LLMs to be good at agentic tasks imposes a list of challenges. Minimax calls it the "Impossible Triangle".
- The Impossible Triangle
- System Throughput: 
- We need to process huge volumes of training data quickly
- Training Stability:
- Training should converge and not oscillate crazily
- Agent Flexibility:
- Diverse range of task where agentic AI is good at
- "Scaffolds"
- Users run agents in all kinds of scaffolds - multi-agent interactions, memory agents, coding sandboxes, etc. The model needs to work well across all of them. 
- Q. I hear the term scaffold a lot - what is this weird term?
- "A scaffold is an external system or wrapper that surrounds the base language model and defines how it operates as an agent. "
- Q. I don't get it, use "scaffold in a sentence"
- "Claude Opus 4.6 is a trained LLM. Claude Code is a scaffold for doing code using Opus. Claude Code orchestrates the the rules, prompts, tools, and makes API requests to Opus."
- Q. Doesn't count you used more than one sentence
- "..."
- "The Forge"
- Image
- OOOOOOOHHHHHH THE FORGGEEEEE!
- Forge is the RL framework Minimax uses to balance all 3 corners of the Impossible Triangle. 
- It tries to solve "how do you train stable agents to perform well in a diverse range of scaffolds and learn from huge volumes of training data cutely?"
- Like most amazing architectures, Forge is also a 3-side architecture. 
- The Agent Side: The external code that decides what to do: manages context, calls tools, chains reasoning steps, and generate trajectories
- Middleware Side: A passthrough that silently intercepts all LLM requests and stores the resulting data in a data pool.
- Engine Side: Contains the LLM engine and the training engine. This is where the model is hosted, it generates tokens and performs weight updates too.
- Q. How do they interact with each other?
- - The agent makes LLM calls. 
- - The call passes through the middleware
- - Middleware routes them to the inference engine for generation
- - Also quietly, the Middleware collects everything into a data pool that the training engine can consume later.
-  - The LLM engine receives the requests,  and generates tokens, sends it back to agent via middleware
- - The training engine consumes processed token sequences from the data pool to update the policy.
- Q. Why is it so special?
- The special part actually comes from this middleware layer. Other frameworks use an agent and a training/LLM engine. Introducing this middleware and data storage layer basically decouples the underlying training-inference engine from the agent.
### tags:
- todo
### comments-github:
- none
### comments-outbound:
- TODO: add a bullet comment here to post to GitHub
### task-dependencies:
### documentation:
### test-condition-1:
- TODO
### test-command:
- TODO
### reviewed:
### tested:
### last-error-types:
### last-error-times:
### log-stream-command:
- TODO
### last-error-loglines:
### notes:
- title: add RLM integration to dialtone agent
- state: OPEN
- author: timcash
- created-at: 2026-02-13T18:52:57Z
- updated-at: 2026-02-14T21:18:54Z
