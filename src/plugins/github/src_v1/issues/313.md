# 313-add-to-the-gemini-plugin-kv-cache-sharing-and-radix-trees-controlled-via-the-cli-and-shared-library
### signature:
- status: wait
- issue: 313
- source: github
- url: https://github.com/timcash/dialtone/issues/313
- synced-at: 2026-02-21T19:50:23Z
### sync:
- github-updated-at: 2026-02-21T18:31:52Z
- last-pulled-at: 2026-02-21T19:50:23Z
- last-pushed-at: 
- github-labels-hash: 
### description:
- 1. The Technology: KV Cache Sharing and Radix Trees
- When an LLM processes text, it generates a mathematical representation of previous tokens called the KV (Key-Value) Cache. Normally, if you send an LLM a long prompt, it computes this cache to understand the context.
- To achieve what you described—spawning subagents to explore different paths without recomputing the parent’s context—AI engineers use Prefix Caching (also enabled by memory management systems like PagedAttention or RadixAttention).
- Here is how it maps to your scenario:
- The Parent Agent: The parent agent processes a long prompt (e.g., system instructions, current state of the problem, past memory). The inference engine computes the KV cache for this context and holds it in GPU memory.
- The Spawning (Branching): When the parent agent needs to explore two paths, it issues two new requests. Both requests share the exact same prefix (the parent's history). The inference engine recognizes this and reuses the parent's KV cache instantly.
- The Divergence: The engine only computes new KV cache entries for the new tokens generated by Subagent A and Subagent B.
- 2. Recursive Branching (Tree of Thoughts / MCTS)
- Yes, you can do this recursively. In systems designed for this (like the open-source SGLang inference engine, which uses a Radix-Tree for the KV Cache), the KV cache literally becomes a branching tree in the GPU memory.
- Root (Parent): Cache node A.
- Branch 1 (Subagent A): Points to Node A, creates new Cache Node B.
- Branch 2 (Subagent B): Points to Node A, creates new Cache Node C.
- Recursive Sub-subagents: If Subagent A spawns two more paths, they point to Node B, creating Nodes D and E.
- This architecture fundamentally enables reasoning frameworks like Tree of Thoughts (ToT) or Monte Carlo Tree Search (MCTS). The model acts like a chess computer: it looks ahead down multiple branching paths, evaluates which outcome looks best, prunes the bad branches, and then chooses the final action for the main agent.
- 3. How this applies to Gemini
- Google supports this architectural concept via Context Caching in their Gemini API.
- If you are using Gemini, you can explicitly create a "Context Cache" containing the parent agent's massive system prompt, uploaded documents, and conversation history.
- Once that cache is created, you can spawn as many parallel "subagents" (concurrent API calls) as you want.
- Every subagent points to that specific cache ID.
- Google’s backend will reuse the computed KV states for that shared context, which makes the subagent responses lightning-fast and significantly cheaper (cached tokens cost a fraction of the price of un-cached tokens).
- To do it recursively with an API:
- If a subagent goes down a path and generates 10,000 tokens of new useful context, and you want to branch again from there, you would update the cache or create a new cache that includes the parent's context + the subagent's new context. Commercial APIs sometimes have minimum token limits for what can be cached (e.g., Gemini originally required a minimum of 32k tokens to initiate a cache), but conceptually, the workflow remains exactly as you described.
- Why this is the future of AI Agents
- By sharing the KV cache recursively, you solve the two biggest bottlenecks in multi-agent systems:
- Speed (Latency): Subagents don't have to re-read the entire history of the task. Their "Time To First Token" (TTFT) drops to milliseconds because the parent already did the reading.
- Cost: You aren't paying the API to process the same 50,000 tokens of background information 15 times when exploring 15 different solutions.
- This shared-memory, recursive branching approach is exactly what allows newer generation models (like OpenAI's "o1/o3" series or Google's advanced reasoning models) to "think" deeply before they speak. They are rapidly spawning internal sub-processes, sharing a mental state (the KV cache), simulating different paths, and collapsing the tree back to the best answer.
### tags:
- todo
### comments-github:
- none
### comments-outbound:
- TODO: add a bullet comment here to post to GitHub
### task-dependencies:
### documentation:
### test-condition-1:
- TODO
### test-command:
- TODO
### reviewed:
### tested:
### last-error-types:
### last-error-times:
### log-stream-command:
- TODO
### last-error-loglines:
### notes:
- title: add to the gemini plugin KV Cache Sharing and Radix Trees controlled via the CLI and shared library
- state: OPEN
- author: timcash
- created-at: 2026-02-21T18:31:52Z
- updated-at: 2026-02-21T18:31:52Z
